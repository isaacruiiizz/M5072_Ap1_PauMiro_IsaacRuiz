# Part 1: ETL i Model de Dades

## Introducci贸 i Objectius
En aquesta fase inicial, l'objectiu 茅s transformar les dades "crues" de MongoDB en un dataset estructurat i net, preparat per a l'aprenentatge automtic. Hem realitzat un proc茅s ETL (Extract, Transform, Load) complet, integrant tecnologies de contenidoritzaci贸 i emmagatzematge al n煤vol.

## 0. Desplegament de la Infraestructura (Docker)
Per garantir la reproductibilitat de l'entorn i l'a茂llament del servei de base de dades, hem implementat una arquitectura contenidoritzada utilitzant **Docker Compose**.

S'ha configurat el servei `mongodb` al fitxer [docker-compose.yaml](../Docker/docker-compose.yaml), definint:
* **Imatge:** `mongo:latest` per disposar de l'煤ltima versi贸 estable.
* **Persist猫ncia:** Configuraci贸 de **Volums** (`mongo_data`) per assegurar que les dades sobrevisquin al reinici del contenidor.
* **Mapeig de Ports:** Exposici贸 del port `27017` per permetre la connexi贸 des del nostre entorn de desenvolupament local (Python).

## 1. Ingesta i Restauraci贸 de Dades
El proc茅s de crrega inicial de dades (Data Seeding) s'ha realitzat a partir d'un arxiu binari `.archive`. Aquest proc茅s consta de dues fases cr铆tiques:

**Fase A: Transfer猫ncia al Contenidor**
Mitjan莽ant la comanda `docker cp`, hem transferit l'arxiu de c貌pia de seguretat des del sistema host fins al directori temporal (`/tmp`) dins del contenidor en execuci贸.

![Exemple de transfer猫ncia al contenidor](Imagenes/Part1/CopiaBackupDocker.png)

**Fase B: Execuci贸 del Mongorestore**
Un cop l'arxiu 茅s accessible pel contenidor, hem executat la utilitat `mongorestore` per reconstruir les col路leccions (`FEB3_players_statistics`, `FEB3_players_shots`, etc.) dins la nostra instncia local.

![Exemple de restauraci贸 de dades](Imagenes/Part1/RestauracioDades.png)

## 2. Configuraci贸 de l'Entorn de Desenvolupament
Per interactuar programticament amb la base de dades, hem establert un entorn de desenvolupament en Python basat en la gesti贸 estricta de depend猫ncies i la modularitat del codi.

**Gesti贸 de Depend猫ncies**
S'ha utilitzat un fitxer [requirements.txt](../Python/requirements.txt) per definir i instal路lar les llibreries necessries (`pymongo`, `pandas`, `azure-storage-blob`, `python-dotenv`), assegurant que tots els membres de l'equip treballin amb les mateixes versions.

![Exemple d'instal路laci贸 de llibreries](Imagenes/Part1/InstalacioLlibreries.png)

**M貌dul de Connexi贸 (Modularitzaci贸)**
Per evitar la duplicaci贸 de codi i millorar la seguretat, hem encapsulat la l貌gica de connexi贸 en el m貌dul [Codi Font: connMongo.py](../Python/Part1/connMongo.py). Aquest script implementa el patr贸 **Singleton** per gestionar la connexi贸 i carrega dinmicament les variables d'entorn (IP, Port) segons si s'executa en local o via VPN (Tailscale).

A continuaci贸, es mostra la validaci贸 de la connexi贸 recuperant els primers registres de la col路lecci贸 d'estad铆stiques:

![Exemple de validaci贸 de connexi贸](Imagenes/Part1/ProvaConnexio.png)

## 3. Definici贸 de l'Arquitectura de Dades

Per tal d'estructurar el flux de dades de manera professional i escalable, hem decidit implementar una **Arquitectura Medallion (Multi-hop Architecture)** utilitzant **Azure Blob Storage** com a Data Lake.

### 3.1. Per qu猫 Azure Blob Storage?
Hem decidit externalitzar l'emmagatzematge al n煤vol (Cloud) per diversos motius t猫cnics:

* **Simulaci贸 d'entorn Big Data real:** En la ind煤stria, les dades no es guarden en local, sin贸 en Data Lakes distribu茂ts.
* **Desacoblament (Decoupling):** Separem la *computaci贸* (el nostre script Python local) de l'*emmagatzematge* (Azure). Aix貌 permet que es puguin accedir a les mateixes dades processades sense haver d'enviar-se fitxers CSV manualment.
* **Tra莽abilitat:** Podem mantenir l'hist貌ric de les dades originals encara que ens equivoquem en el processament posterior.

### 3.2. Estructura de Capes (The Medallion Architecture)

El nostre pipeline ETL mou les dades a trav茅s de tres contenidors al n煤vol:

#### Capa BRONZE (Raw Zone)
* **Descripci贸:** s la "zona d'aterratge". Cont茅 una c貌pia exacta de les dades tal com s'han extret de MongoDB, sense cap modificaci贸.
* **Format:** Fitxer `.csv` amb totes les columnes originals.
* **Funci贸:** Actua com a c貌pia de seguretat immutable. Si cometem un error en la neteja, sempre podem tornar a aquesta capa per regenerar el dataset sense haver de tornar a consultar la base de dades.
* **Contingut:** Totes les temporades i competicions (FEB3/EBA) amb registres "bruts" (inclosos jugadors amb 0 minuts, noms duplicats o dades err貌nies).

#### Capa SILVER (Clean Zone)
* **Descripci贸:** Dades netes, filtrades i tipificades.
* **Transformacions aplicades:**
    * **Filtratge de qualitat:** Eliminaci贸 de jugadors amb menys de X minuts o partits jugats (per evitar distorsions estad铆stiques per *small sample size*).
    * **Neteja d'estructures:** Eliminaci贸 de columnes innecessries (IDs interns de Mongo) i estandarditzaci贸 de noms d'equips i temporades.
    * **Gesti贸 de Nuls:** Imputaci贸 de valors `0` on hi ha `NaN` en estad铆stiques de comptatge.
* **Contingut:** Una taula estructurada on cada fila 茅s un jugador/temporada vlid per a l'anlisi.

#### Capa GOLD (Business / ML Zone)
* **Descripci贸:** Dades enriquides i llestes per alimentar el model de Machine Learning.
* **Feature Engineering:** En aquesta capa 茅s on apliquem el coneixement de domini (bsquet) per crear noves m猫triques que no existien a l'origen:
    * **OER (Offensive Efficiency Rating):** Punts per possessi贸.
    * **Possessions Estimades:** F贸rmula avan莽ada per calcular el volum de joc real.
    * **% Volum de Tirs:** Distribuci贸 de tirs (Triples vs Tirs de 2).
* **Contingut final:** Aquest 茅s el dataset (`final_dataset.csv`) que llegir l'algorisme **K-Means** a la Part 2 del projecte.

### 3.3. Esquema del Pipeline

```mermaid
graph LR
    A[("MongoDB (Origen)")] -->|Extract| B(Python Script)
    B -->|Load Raw| C[("Azure Blob: BRONZE")]
    C -->|Clean & Filter| D[("Azure Blob: SILVER")]
    D -->|Feature Engineering| E[("Azure Blob: GOLD")]
    E -->|Input| F{Model K-Means}
    
    style C fill:#cd7f32,stroke:#333,stroke-width:2px,color:white
    style D fill:#c0c0c0,stroke:#333,stroke-width:2px,color:black
    style E fill:#ffd700,stroke:#333,stroke-width:2px,color:black
```

## 4. Creaci贸 i Configuraci贸 del Data Lake a Azure

Per allotjar la nostra arquitectura Medallion, hem creat un recurs de tipus **Azure Storage Account**. Dins d'aquest compte, hem hagut de prendre una decisi贸 d'arquitectura important sobre com organitzar les dades: utilitzar un sol contenidor amb subcarpetes o m煤ltiples contenidors.

### 4.1. Estrat猫gia de Contenidors
Hem optat per crear **3 contenidors independents** en lloc de carpetes dins d'un de sol. Aquesta decisi贸 s'alinea amb els estndards de la ind煤stria del Big Data per tres motius:

1.  **Seguretat i RBAC (Role-Based Access Control):** Permet definir pol铆tiques d'acc茅s granulars. En un entorn real, podr铆em donar acc茅s de "Nom茅s Lectura" als analistes de dades sobre el contenidor `gold`, sense que tinguin perm铆s per modificar o esborrar les dades crues del contenidor `bronze`.
2.  **Cicle de Vida de les Dades:** Azure permet configurar regles automtiques per contenidor. Per exemple, podr铆em programar que les dades de `bronze` (molt pesades) s'esborrin automticament passats 30 dies, mentre que les de `gold` es mantinguin indefinidament.
3.  **Compatibilitat:** Aquesta estructura facilita la integraci贸 futura amb eines com Databricks o Azure Synapse.

Hem creat la seg眉ent estructura al Portal d'Azure:

* **`01-bronze`**: Recepci贸 de dades crues (Raw Data).
* **`02-silver`**: Dades netes i filtrades.
* **`03-gold`**: Dades agregades i llestes per al modelatge.

![Vista dels contenidors creats al compte d'emmagatzematge](Imagenes/Part1/AzureContainers.png)

### 4.2. Gesti贸 de Credencials i Seguretat
Per connectar el nostre script Python local amb el n煤vol, utilitzem les **Access Keys** (Claus d'acc茅s) proporcionades per Azure.

Per mantenir la seguretat del projecte i evitar pujar credencials sensibles al repositori de codi (GitHub), hem implementat l'煤s de variables d'entorn:
1.  Hem copiat la `Connection String` d'Azure.
2.  L'hem desat al fitxer local `.env` sota la variable `AZURE_CONNECTION_STRING`.
3.  Hem configurat `.gitignore` per excloure aquest fitxer.

![Obtenci贸 de la cadena de connexi贸](Imagenes/Part1/AzureKeys.png)

## 5. Implementaci贸 arquitectura Medallion

### 5.1. Implementaci贸 de la Capa Bronze (Ingesta)

El primer pas del pipeline 茅s l'extracci贸 de les dades de MongoDB i la seva crrega a la capa **Bronze**. Per fer-ho, hem desenvolupat l'script `extraccioDades.py`.

#### 5.1.1. L貌gica del Script
L'script s'ha dissenyat per ser eficient i no ocupar espai al disc local:
1.  **Iteraci贸:** Processa automticament les col路leccions `statistics` i `shots`.
2.  **In-Memory Buffer:** Utilitza `io.StringIO` per convertir les dades a CSV directament a la RAM i pujar-les a Azure via streaming, sense crear fitxers temporals.
3.  **Seguretat:** Utilitza variables d'entorn per a l'autenticaci贸.

Podeu consultar el codi font complet a l'enlla莽 seg眉ent:

**[Codi Font: extraccioDades.py](../Python/Part1/extraccioDades.py)**

#### 5.1.2. Execuci贸 i Validaci贸
L'execuci贸 de l'script mostra la connexi贸 correcta, la descrrega de m茅s de 200.000 registres i la pujada exitosa al n煤vol.

![Execuci贸 de l'script al terminal](Imagenes/Part1/ExecucioExtraccio.png)

Finalment, validem al Portal d'Azure que els fitxers `feb_raw_shots.csv` i `feb_raw_statistics.csv` s'han creat correctament dins el contenidor `01-bronze`.

![Validaci贸 al Portal Azure](Imagenes/Part1/AzureBronzeResults.png)

### 5.2. Implementaci贸 de la Capa Silver (Transformaci贸)

La capa **Silver** actua com a "Single Source of Truth" (Font nica de Veritat) del projecte. L'objectiu d'aquesta fase 茅s transformar les dades crues i desestructurades de la capa Bronze en una taula anal铆tica neta, consistent i agregada.

#### 5.2.1. Estrat猫gia d'Agregaci贸 (Canvi de Granularitat)
A la capa Bronze, la granularitat de les dades 茅s "per partit" (1 fila = 1 actuaci贸 d'un jugador en un partit o en un tipus de tir). Per al nostre objectiu final (Clustering de Rols amb K-Means), necessitem analitzar el comportament global del jugador durant tota la campanya.

Per tant, a la capa Silver realitzem una **agregaci贸 `GROUP BY`** per:
1.  **Jugador (`player_feb_id`)**
2.  **Temporada (`season_name`)**

D'aquesta manera, passem de tenir ~200.000 registres de partits a un dataset de ~15.000 files 煤niques (una per jugador/temporada), sumant totes les seves estad铆stiques acumulades.

#### 5.2.2. Gesti贸 d'Evoluci贸 de l'Esquema (Schema Evolution)
Un repte detectat 茅s que les diferents temporades de la FEB no sempre tenen les mateixes columnes (per exemple, temporades antigues no registraven els "Taps Rebuts" o `blka`).

Per solucionar-ho, hem implementat un patr贸 de **"Master Schema"**:
* Definim un diccionari de configuraci贸 (`STATS_CONFIG`) que llista estrictament les variables que volem al dataset final.
* Si una temporada no cont茅 una variable, l'script la crea artificialment i l'omple amb `0`.
* Aix貌 garanteix que el dataset resultant tingui sempre la mateixa estructura, independentment de l'any de les dades.

#### 5.2.3. Selecci贸 de Variables (Feature Selection)
De les m茅s de 60 columnes disponibles a l'origen, hem seleccionat les seg眉ents per definir el perfil t猫cnic del jugador, descartant metadades irrellevants (hora del partit, jornades) o m猫triques derivades (percentatges) que recalcularem a la capa Gold.

##### A. M猫triques de Volum i Anotaci贸
Variables necessries per determinar la importncia del jugador en l'atac.
* **Volum:** `minutes`, `starter` (partits titular), `games_played`.
* **Anotaci贸:** `pts`, `2pm`/`2pa` (Tirs de 2), `3pm`/`3pa` (Triples), `ftm`/`fta` (Tirs lliures), `dunk` (Esmaixades).

##### B. M猫triques de Generaci贸 i "Hustle"
Variables que defineixen rols defensius o de creaci贸 de joc.
* **Control:** `ast` (Assist猫ncies), `tov` (P猫rdues).
* **Defensa/Lluita:** `orb` (Rebot Ofensiu), `drb` (Defensiu), `stl` (Robatoris), `blk` (Taps), `pf` (Faltes).

##### C. M猫triques Espacials (Shot Chart)
Per diferenciar rols moderns (ex: *Corner Specialist* vs *Rim Runner*), hem extret les dades de localitzaci贸 de tir proporcionades per la FEB (`rc_*`), agrupant-les en zones tctiques:
* **Pintura (Paint):** Suma de tirs a zona restringida (`rc_pc`, `rc_pl`, `rc_pr`).
* **Mitja Distncia (Mid-Range):** Suma de tirs a mitja distncia (`rc_mel`, `rc_mer`, etc.).
* **Triples:** Diferenciaci贸 entre triples de cantonada (`rc_c3l`, `rc_c3r`) i frontals.

#### 5.2.4. Pol铆tica de Neteja de Dades (Data Cleaning)
S'ha aplicat una estrat猫gia defensiva per garantir la qualitat aritm猫tica de les dades:

1.  **Eliminaci贸 d'Orfes:** S'eliminen els registres que no tenen `player_feb_id` o `player_name`, ja que no es poden atribuir a cap entitat.
2.  **Imputaci贸 de Nuls:** Tots els valors nuls (`NaN`) en columnes num猫riques es substitueixen per `0`. Assumim que l'abs猫ncia de dada en una estad铆stica de comptatge equival a que no s'ha produ茂t l'acci贸.
3.  **Filtratge de Soroll:** S'eliminen els jugadors amb menys de **50 minuts totals** a la temporada, ja que les seves dades s贸n estad铆sticament irrellevants i podrien distorsionar els cl煤sters.

#### 5.2.5. Implementaci贸 i Validaci贸 de la Crrega

Per materialitzar l'estrat猫gia definida, hem desenvolupat i executat l'script `transformacioDades.py`. Aquest codi orquestra tot el proc茅s: descarrega les dades de Bronze, aplica la normalitzaci贸 de l'esquema, neteja els nuls i agrega els registres per temporada.

Podeu consultar el codi font complet a l'enlla莽 seg眉ent:

**[ Codi Font: transformacioDades.py](../Python/Part1/transformacioDades.py)**

#### A. Execuci贸 de l'Script
En executar el codi, el sistema processa les col路leccions d'estad铆stiques i tirs. Com es pot veure a la sortida del terminal, el filtre de qualitat actua correctament, reduint el nombre de registres per quedar-nos nom茅s amb els jugadors rellevants (>50 minuts).

![Execuci贸 de l'script de transformaci贸](Imagenes/Part1/ExecucioTransformacio.png)

#### B. Persist猫ncia al Data Lake
Un cop finalitzat el proc茅s, verifiquem al Portal d'Azure que el fitxer resultant `feb_silver_dataset.csv` s'ha creat correctament dins del contenidor `02-silver`. Aquest fitxer actua ara com la nostra font de veritat neta.

![Validaci贸 del fitxer al contenidor Silver](Imagenes/Part1/AzureSilverResults.png)

#### C. Auditoria de Dades (Data Quality Check)
Finalment, realitzem una inspecci贸 del contingut del fitxer generat per validar que s'han complert les regles de negoci:
1.  **Estructura:** El dataset cont茅 **12.708 files** (jugadors 煤nics per temporada) i **43 columnes** normalitzades.
2.  **Integritat:** S'ha verificat que **no existeixen valors nuls** (Total Nulls = 0), confirmant que l'estrat猫gia d'imputaci贸 ha funcionat.
3.  **Dades Espacials:** Les columnes de zones de tir (`rc_c3l_a`, `rc_pc_a`, etc.) estan correctament poblades.

![Vista pr猫via del dataset Silver net](Imagenes/Part1/SilverContentPreview.png)


